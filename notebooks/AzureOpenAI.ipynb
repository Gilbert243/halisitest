{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_fPIxKgSVtE"
   },
   "source": [
    "### **Chat Completion and Text Embedding using Azure OpenAI**\n",
    "This notebook explores the OpenAI language models for chat completion and text embedding tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mU89dzDKfLJ2"
   },
   "source": [
    "**STEP 0**: Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vnZ-znTQSZ1h",
    "outputId": "dea1fc24-67d5-488a-e5c9-03b7a7edc827"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages (1.43.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages (from openai) (2.6.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\deogratias lukamba\\appdata\\roaming\\python\\python310\\site-packages (from httpx<1,>=0.23.0->openai) (2022.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\deogratias lukamba\\anaconda3\\envs\\stlit\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "%pip install openai\n",
    "%pip install numpy\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjhcnFOpfRkv"
   },
   "source": [
    "**STEP 1**: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NPxGMUkfSb-Y"
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import time\n",
    "from openai import AzureOpenAI\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kLmmVmhfUmU"
   },
   "source": [
    "**STEP 2**: Create Azure OpenAI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('AZUREOPENAI_API_KEY')\n",
    "api_version = os.getenv('AZUREOPENAI_API_VERSION')\n",
    "azure_endpoint = os.getenv('AZUREOPENAI_API_ENDPOINT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Azure OpenAI client\n",
    "# Make sure the environment variables are created\n",
    "client = AzureOpenAI(\n",
    "    api_key=api_key,\n",
    "    api_version=api_version,\n",
    "    azure_endpoint = azure_endpoint\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDOW4V1DfZjL"
   },
   "source": [
    "**STEP 3**: Define chat completion and text embedding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rrluIwg3ju4d"
   },
   "outputs": [],
   "source": [
    "# Define chat completion function\n",
    "def completeChat(prompt, style, client, model=\"gpt-35-turbo\"):\n",
    "    # Execute API call\n",
    "    result = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages= [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": style,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "        ],\n",
    "        max_tokens=800,\n",
    "        temperature=0.2,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None,\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    # Extract the response\n",
    "    response = result.choices[0].message.content\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OK3cxRkkItBP"
   },
   "outputs": [],
   "source": [
    "# Define text embedding function\n",
    "def embedText(text, client, model=\"text-embedding-ada-002\"):\n",
    "    # Execute API call\n",
    "    result = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "\n",
    "    # Extract and normalize the embeddings\n",
    "    embedding = np.array(result.data[0].embedding)\n",
    "    embedding /= np.linalg.norm(embedding)\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIq9zn-Zfhcu"
   },
   "source": [
    "**STEP 4**: Create prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nKbwiqYdUtc2"
   },
   "outputs": [],
   "source": [
    "# Create prompt\n",
    "style = \"You are a helpful Digital Signal Processing expert.\"\n",
    "first_topic = \"Particle Filtering\"\n",
    "second_topic = \"Particle Flow Filtering\"\n",
    "prompt = f\"\"\"\n",
    "What are the differences between {first_topic} and {second_topic}?\n",
    "Please provide a detailed comparison including:\n",
    "- Common points\n",
    "- Main differences\n",
    "\n",
    "\n",
    "Write a little python code snippet with examples for both methods.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vN9rL0SkfkYf"
   },
   "source": [
    "**STEP 5**: Execute chat completion task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GFg9oWLUH5LQ",
    "outputId": "5722e16e-f504-47aa-9155-ee36aae3fb95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Particle Filtering and Particle Flow Filtering are both methods used for state estimation in dynamic systems. Here's a detailed comparison of the two methods:\n",
      "\n",
      "Common Points:\n",
      "1. Both methods are based on the concept of using a set of particles to represent the state of the system and updating their weights based on measurements.\n",
      "2. They are both used for non-linear and non-Gaussian state estimation problems.\n",
      "3. Both methods are iterative and involve a prediction step followed by a measurement update step.\n",
      "\n",
      "Main Differences:\n",
      "1. Particle Filtering:\n",
      "   - In particle filtering, the particles are propagated through the system using the system dynamics model, and their weights are updated based on the likelihood of the measurements given the predicted state.\n",
      "   - It is a sequential Monte Carlo method and is also known as Sequential Importance Sampling (SIS).\n",
      "   - Particle filtering can suffer from sample degeneracy and particle impoverishment, especially in high-dimensional state spaces.\n",
      "\n",
      "2. Particle Flow Filtering:\n",
      "   - Particle Flow Filtering is an extension of particle filtering that uses a flow-based approach to update the particles' weights, allowing for more efficient exploration of the state space.\n",
      "   - It uses a series of flow transformations to update the particles' weights, which can lead to better exploration of the posterior distribution and reduce the sample degeneracy and particle impoverishment issues.\n",
      "   - Particle Flow Filtering is also known as Sequential Monte Carlo with Particle Flow (SMC-PF).\n",
      "\n",
      "Here's a Python code snippet with examples for both methods:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from scipy.stats import norm\n",
      "\n",
      "# Particle Filtering Example\n",
      "def particle_filtering(system_model, measurement_model, num_particles, initial_state, measurements):\n",
      "    particles = np.random.normal(initial_state, 1, size=(num_particles,))\n",
      "    for measurement in measurements:\n",
      "        # Prediction step\n",
      "        particles = system_model(particles)\n",
      "        # Measurement update step\n",
      "        weights = measurement_model(measurement, particles)\n",
      "        weights /= np.sum(weights)\n",
      "        particles = np.random.choice(particles, size=num_particles, replace=True, p=weights)\n",
      "    return particles\n",
      "\n",
      "# Particle Flow Filtering Example\n",
      "def particle_flow_filtering(system_model, measurement_model, num_particles, initial_state, measurements):\n",
      "    particles = np.random.normal(initial_state, 1, size=(num_particles,))\n",
      "    for measurement in measurements:\n",
      "        # Prediction step\n",
      "        particles = system_model(particles)\n",
      "        # Particle Flow update step\n",
      "        weights = measurement_model(measurement, particles)\n",
      "        weights /= np.sum(weights)\n",
      "        ess = 1 / np.sum(weights ** 2)\n",
      "        if ess < num_particles / 2:\n",
      "            particles = resample_particles(particles, weights)\n",
      "    return particles\n",
      "\n",
      "def system_model(particles):\n",
      "    return particles + np.random.normal(0, 1, size=particles.shape)\n",
      "\n",
      "def measurement_model(measurement, particles):\n",
      "    return norm.pdf(measurement, particles, 1)\n",
      "\n",
      "def resample_particles(particles, weights):\n",
      "    indices = np.random.choice(np.arange(len(particles)), size=len(particles), replace=True, p=weights)\n",
      "    return particles[indices]\n",
      "\n",
      "# Example usage\n",
      "initial_state = 0\n",
      "measurements = np.random.normal(0, 1, size=10)\n",
      "\n",
      "# Particle Filtering\n",
      "pf_result = particle_filtering(system_model, measurement_model, 1000, initial_state, measurements)\n",
      "\n",
      "# Particle Flow Filtering\n",
      "pff_result = particle_flow_filtering(system_model, measurement_model, 1000, initial_state, measurements)\n",
      "```\n",
      "\n",
      "In this example, `system_model` represents the dynamics of the system, `measurement_model` represents the likelihood of the measurements given the state, and `resample_particles` is a function to perform resampling if the effective sample size falls below a threshold in the Particle Flow Filtering method.\n",
      "Time taken: 6.85 seconds\n"
     ]
    }
   ],
   "source": [
    "# Execute chat completion\n",
    "start_time = time.time()\n",
    "completion_model = \"gpt-35-turbo\" # Make sure it is deployed on Azure AI Studio\n",
    "response = completeChat(prompt, style, client, completion_model)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Print the response\n",
    "print(response)\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_RrN6GMfqv3"
   },
   "source": [
    "**STEP 6**: Execute text embedding task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8uOe6ujqj_Ab",
    "outputId": "89a621b6-977c-408d-8667-c3c1bf3335a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01215332  0.02305597 -0.03151163 ... -0.01013457 -0.01059677\n",
      " -0.04551378]\n",
      "(1536,)\n",
      "1.0\n",
      "Time taken: 0.12 seconds\n"
     ]
    }
   ],
   "source": [
    "# Execute text emebdding on response from chat completion\n",
    "start_time = time.time()\n",
    "embedding_model = \"text-embedding-ada-002\" # Make sure it is deployed on Azure AI Studio\n",
    "embedding = embedText(response, client, embedding_model)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Print the embeddings\n",
    "print(embedding)\n",
    "print(embedding.shape)\n",
    "print(np.linalg.norm(embedding))\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlJpj2Y7fui2"
   },
   "source": [
    "**STEP 7**: Compute similarity score between query and response embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XIKakGP9UMWA",
    "outputId": "1bccf9fc-7533-46ab-d120-e8700f435c3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7988360467541769\n",
      "Time taken: 0.19 seconds\n"
     ]
    }
   ],
   "source": [
    "# Compare chat completion response to a query\n",
    "start_time = time.time()\n",
    "first_keyword = \"Bayesian Filtering\"\n",
    "second_keyword = \"State Estimation\"\n",
    "query = f\"Does the response talk about {first_keyword} and {second_keyword}?\"\n",
    "query_embedding = embedText(query, client, embedding_model)\n",
    "similarity = np.dot(embedding, query_embedding) # Compute cosine similarity via inner product\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Print the similarity result\n",
    "print(similarity)\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "stlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
