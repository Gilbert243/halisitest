# Guide d'Utilisation - Notebook d'Analyse de Performance

## üìì Nouveau Notebook `run.ipynb`

Le notebook a √©t√© compl√®tement refait avec une structure claire et professionnelle.

## üéØ Caract√©ristiques Principales

### 1. **Dataset √âquilibr√©**
- √âchantillonnage de **10 images par classe** pour √©viter le biais d√ª au d√©s√©quilibre
- Comparaison entre dataset original et dataset √©quilibr√©
- Total: 100 images (10 classes √ó 10 images)

### 2. **Analyse Compl√®te**
Le notebook contient 12 √©tapes:

1. **Import Libraries** - Configuration de l'environnement
2. **Load Configuration** - Chargement des param√®tres API
3. **Original Dataset Analysis** - Analyse du dataset complet
4. **Balanced Dataset Creation** - Cr√©ation du dataset √©quilibr√© (10/classe)
5. **System Initialization** - Configuration du classifier
6. **Run Evaluation** - √âvaluation sur le dataset √©quilibr√©
7. **Overall Metrics** - M√©triques globales (accuracy, F1, etc.)
8. **Confusion Matrix** - Matrices de confusion (absolue et normalis√©e)
9. **Per-Class Metrics** - M√©triques d√©taill√©es par classe
10. **Latency Analysis** - Analyse de la latence et throughput
11. **Error Analysis** - Analyse des erreurs de classification
12. **Summary Report** - Rapport complet pour votre document

### 3. **Visualisations de Qualit√©**
- Graphiques en barres et camemberts pour la distribution
- Matrices de confusion color√©es
- Graphiques de performance par classe
- Histogrammes et box plots de latence
- Heatmaps d'erreurs

## üöÄ Comment Utiliser

### √âtape 1: V√©rifier l'environnement

```bash
# Activer l'environnement virtuel
source env/bin/activate

# V√©rifier que les variables d'environnement sont configur√©es
echo $AZUREOPENAI_API_KEY
echo $AZUREOPENAI_API_ENDPOINT
```

### √âtape 2: Lancer le notebook

Ouvrez `run.ipynb` dans VS Code ou Jupyter et **ex√©cutez les cellules s√©quentiellement**.

### √âtape 3: Dur√©e d'ex√©cution

- **Cellules 1-5**: ~10 secondes (configuration)
- **Cellule 6**: ~3-5 minutes (√©valuation de 100 images)
- **Cellules 7-12**: ~30 secondes (analyses et visualisations)

**Temps total estim√©: ~5-7 minutes**

## üìä R√©sultats Obtenus

### Fichiers G√©n√©r√©s

1. **`data/annotations_balanced.csv`** - Dataset √©quilibr√© (10 images/classe)
2. **`results_evaluation_YYYYMMDD_HHMMSS.csv`** - R√©sultats d√©taill√©s avec pr√©dictions

### M√©triques Calcul√©es

#### M√©triques Globales
- ‚úÖ Overall Accuracy
- ‚úÖ Weighted F1-Score (tient compte du d√©s√©quilibre)
- ‚úÖ Macro F1-Score (moyenne simple)
- ‚úÖ Weighted Precision & Recall

#### M√©triques par Classe
- ‚úÖ Precision par classe
- ‚úÖ Recall par classe
- ‚úÖ F1-Score par classe
- ‚úÖ Support (nombre d'√©chantillons)

#### M√©triques de Latence
- ‚úÖ Mean, Median, Std Dev
- ‚úÖ P25, P50, P75, P95, P99
- ‚úÖ Min, Max
- ‚úÖ Throughput (req/s)

#### Analyse d'Erreurs
- ‚úÖ Patterns de confusion les plus fr√©quents
- ‚úÖ Heatmap des erreurs
- ‚úÖ Liste d√©taill√©e des erreurs

## üìà Avantages du Dataset √âquilibr√©

### Pourquoi 10 images par classe?

**Probl√®me avec le dataset original:**
- 1A: 25 images (16.7%)
- 2C: 4 images (2.7%)
- **Ratio de d√©s√©quilibre: 6.25:1**

**Solution avec le dataset √©quilibr√©:**
- Toutes les classes: 10 images chacune (10%)
- **Ratio de d√©s√©quilibre: 1:1** ‚úÖ

### B√©n√©fices

1. **M√©triques Plus Fiables**
   - Chaque classe contribue √©galement
   - Pas de biais vers les classes surrepr√©sent√©es
   - F1-score macro devient pertinent

2. **Comparaison √âquitable**
   - M√™me nombre d'√©chantillons pour chaque classe
   - Performance r√©elle du mod√®le visible
   - Facilite la comparaison entre APIs

3. **Temps d'√âvaluation Raisonnable**
   - 100 images au lieu de 150
   - ~5 minutes au lieu de ~8 minutes
   - R√©sultats statistiquement significatifs

## üìù Utilisation pour le Rapport

### Section 1: Dataset
Utilisez les cellules 3-4:
- Tableau de distribution original
- Graphiques de comparaison (original vs √©quilibr√©)
- Statistiques de d√©s√©quilibre

**√Ä mentionner:**
> "Pour √©viter les biais dus au d√©s√©quilibre des classes (ratio 6.25:1), nous avons cr√©√© un dataset √©quilibr√© avec 10 images par classe, permettant une √©valuation plus juste des performances du mod√®le."

### Section 2: M√©triques de Classification
Utilisez les cellules 7-9:
- Tableau des m√©triques globales
- Matrice de confusion
- Graphiques de performance par classe

**Screenshots √† inclure:**
- Confusion matrix (absolue et normalis√©e)
- Graphiques de Precision/Recall/F1 par classe

### Section 3: Analyse de Latence
Utilisez les cellules 10:
- Statistiques de latence d√©taill√©es
- Graphiques de distribution
- Calculs de throughput

**M√©triques cl√©s:**
- Mean latency
- P95 latency (pour SLA)
- Throughput (req/s)

### Section 4: Analyse d'Erreurs
Utilisez la cellule 11:
- Patterns de confusion
- Heatmap des erreurs
- Interpr√©tation

### Section 5: R√©sum√© Ex√©cutif
Utilisez la cellule 12:
- Rapport complet format√©
- Tous les chiffres cl√©s
- Top/worst performing classes

## üîÑ Tester Plusieurs APIs

Pour comparer diff√©rentes APIs:

1. **Modifier `config/eval.yaml`:**
```yaml
api:
  provider: google_gemini  # ou anthropic_claude
  model: gemini-1.5-pro
```

2. **Cr√©er un nouveau client** dans le notebook (cellule 6):
```python
# Pour Gemini (exemple)
from google.generativeai import GenerativeModel
client = GeminiClient(api_key=os.getenv("GEMINI_API_KEY"))
```

3. **R√©ex√©cuter le notebook**

4. **Comparer les r√©sultats**:
   - Accuracy
   - F1-scores
   - Latence
   - Patterns d'erreurs

## üêõ D√©pannage

### Erreur: API Key non trouv√©e
```bash
# V√©rifier le .env
cat .env

# Ou d√©finir directement
export AZUREOPENAI_API_KEY="votre-cl√©"
```

### Erreur: Module non trouv√©
```bash
pip install -r requirements.txt
```

### Erreur: Timeout
Augmenter le timeout dans `config/eval.yaml`:
```yaml
api:
  timeout: 60  # au lieu de 30
```

## ‚úÖ Checklist Avant Ex√©cution

- [ ] Environnement virtuel activ√©
- [ ] Variables d'environnement d√©finies (.env charg√©)
- [ ] Fichier `data/annotations.csv` pr√©sent
- [ ] Fichier `prompts/vision_only.txt` pr√©sent
- [ ] Fichier `config/eval.yaml` configur√©
- [ ] Connexion internet stable

## üìä Structure du Code

Toutes les fonctions utilisent les bonnes appellations:
- ‚úÖ `EvalConfig` pour la configuration
- ‚úÖ `AzureLLMClient` pour le client API
- ‚úÖ `HairClassifier` pour le classificateur
- ‚úÖ `Evaluator` pour l'√©valuation
- ‚úÖ Colonnes CSV: `image_path`, `type`
- ‚úÖ R√©sultats: dictionnaire avec cl√©s `accuracy`, `weighted_f1`, `confusion_matrix`, etc.

## üéì Code de Qualit√©

- Code comment√© et document√©
- Structure logique en 12 √©tapes
- Gestion d'erreurs incluse
- Visualisations professionnelles
- Messages informatifs pour l'utilisateur

---

**Pr√™t √† lancer?** Ouvrez `run.ipynb` et ex√©cutez toutes les cellules! üöÄ

**Questions?** Consultez [EVALUATION_GUIDE.md](EVALUATION_GUIDE.md) pour plus de d√©tails.
